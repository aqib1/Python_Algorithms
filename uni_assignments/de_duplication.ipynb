{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EYTzvuOYDAX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_files = [\n",
        "    \"sample_data/acm.csv\",\n",
        "    \"sample_data/ieee.csv\",\n",
        "    \"sample_data/wos.csv\",\n",
        "    \"sample_data/scopus.csv\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "KkTzxUhzavd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "possible_title_cols = [\n",
        "    \"Title\",\n",
        "    \"Document Title\",\n",
        "    \"Article Title\",\n",
        "    \"Paper Title\",\n",
        "    \"TI\",          # Some databases use short codes\n",
        "    \"doc_title\"\n",
        "]"
      ],
      "metadata": {
        "id": "Ffmdpq5ubRd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_title_column(df):\n",
        "    # find the first matching title column\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [p.lower() for p in possible_title_cols]:\n",
        "            df[\"title_clean\"] = df[col].astype(str)\n",
        "            return df\n",
        "\n",
        "    # If no known title column found â†’ fallback: create empty column\n",
        "    df[\"title_clean\"] = \"\"\n",
        "    return df"
      ],
      "metadata": {
        "id": "PxvypKejbrGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = []\n",
        "\n",
        "for path in csv_files:\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "        df[\"__source_file\"] = path\n",
        "\n",
        "        # Normalize title column\n",
        "        df = normalize_title_column(df)\n",
        "\n",
        "        merged.append(df)\n",
        "        print(f\"Loaded: {path} | Rows: {len(df)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load {path}: {e}\")\n",
        "\n",
        "merged_df = pd.concat(merged, ignore_index=True)\n",
        "print(f\"\\nTotal merged rows: {len(merged_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3FZ0uFJdCuK",
        "outputId": "9397344b-ec6d-4377-82f2-dcea9ce6809d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: sample_data/acm.csv | Rows: 20\n",
            "Loaded: sample_data/ieee.csv | Rows: 56\n",
            "Loaded: sample_data/wos.csv | Rows: 40\n",
            "Loaded: sample_data/scopus.csv | Rows: 22\n",
            "\n",
            "Total merged rows: 138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_title(text):\n",
        "    text = str(text).lower().strip()\n",
        "    text = re.sub(r\"[^a-z0-9 ]\", \" \", text)   # remove punctuation\n",
        "    text = re.sub(r\"\\s+\", \" \", text)          # collapse multiple spaces\n",
        "    return text"
      ],
      "metadata": {
        "id": "jzEKjEzldFt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df[\"title_clean\"] = merged_df[\"title_clean\"].apply(clean_title)"
      ],
      "metadata": {
        "id": "YA3QwM2QdNHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_mask = merged_df.duplicated(subset=[\"title_clean\"], keep=False)\n",
        "duplicates_only = merged_df[duplicate_mask].copy()\n",
        "deduped_df = merged_df.drop_duplicates(subset=[\"title_clean\"], keep=\"first\").copy()"
      ],
      "metadata": {
        "id": "YgeIBDH2dQhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deduped_df.to_csv(\"/content/deduped_output.csv\", index=False)\n",
        "duplicates_only.to_csv(\"/content/duplicates_found.csv\", index=False)"
      ],
      "metadata": {
        "id": "Mpl55vyJdaWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nðŸ“Š SUMMARY\")\n",
        "print(\"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\")\n",
        "print(f\"Total merged records:     {len(merged_df)}\")\n",
        "print(f\"Unique after dedupe:      {len(deduped_df)}\")\n",
        "print(f\"Duplicate records found:  {len(duplicates_only)}\")\n",
        "print(\"\\nFiles saved:\")\n",
        "print(\"âœ” /content/deduped_output.csv\")\n",
        "print(\"âœ” /content/duplicates_found.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmYcYy3LdcVj",
        "outputId": "7406e4e9-8b83-4671-898d-f92355877809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š SUMMARY\n",
            "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
            "Total merged records:     138\n",
            "Unique after dedupe:      94\n",
            "Duplicate records found:  64\n",
            "\n",
            "Files saved:\n",
            "âœ” /content/deduped_output.csv\n",
            "âœ” /content/duplicates_found.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the deduplicated dataset from the previous step\n",
        "deduped_df = pd.read_csv(\"/content/deduped_output.csv\")\n",
        "\n",
        "print(\"Columns in your data:\")\n",
        "print(list(deduped_df.columns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sbq1aGy7dhKK",
        "outputId": "042bd8c1-6680-436d-b11c-569a65606927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in your data:\n",
            "['Item type', 'Authors', 'Title', 'Journal', 'Publication year', 'Volume', 'Issue', 'Pages', 'Publisher', 'Address', 'Proceedings title', 'Conference location', 'Date published', 'ISBN', 'ISSN', 'URLs', 'DOI', 'Abstract', 'Keywords', 'Series', '__source_file', 'title_clean', 'Document Title', 'Author Affiliations', 'Publication Title', 'Date Added To Xplore', 'Publication Year', 'Start Page', 'End Page', 'ISBNs', 'Funding Information', 'PDF Link', 'Author Keywords', 'IEEE Terms', 'Mesh_Terms', 'Article Citation Count', 'Patent Citation Count', 'Reference Count', 'License', 'Online Date', 'Issue Date', 'Meeting Date', 'Document Identifier', 'Publication Type', 'Book Authors', 'Group Authors', 'Book Group Authors', 'Researcher Ids', 'ORCIDs', 'Book Editors', 'Author - Arabic', 'Grant Principal Investigator', 'Grant Co Principal Investigator', 'Article Title', 'Article Title - SciELO', 'Article Title - SciELO.1', 'Article Title - Chinese', 'Article Title - Russian', 'Patent Number', 'Patent Assignee', 'Source Title - Arabic', 'Source Title', 'Source Title - Korean', 'Book Series Title', 'Book Series Subtitle', 'Special Issue', 'Meeting Abstract', 'Article Number', 'Version', 'Version History', 'Book DOI', 'License Name', 'License URI', 'License Description', 'Early Access Date', 'Supplement', 'Document Type', 'Publication Date', 'Abstract - Foreign', 'Abstract - English Transliteration', 'Abstract - Foreign.1', 'Abstract - Korean', 'Conference Title', 'Conference Date', 'Conference Sponsor', 'Conference Location', 'Times Cited, WoS Core', 'Times Cited, CSCD', 'Times Cited, RSCI', 'Times Cited, ARCI', 'Times Cited, BCI', 'Times Cited, SCIELO', 'Times Cited, All Databases', '180 Day Usage Count', 'Since 2013 Usage Count', 'eISSN', 'Grant Number', 'No of References', 'Cited References', 'Language', 'Advisor', 'Committee Member', 'Copyright', 'Degree Name', 'Institution Address', 'Institution', 'Dissertation and Thesis Subjects', 'Indexed Date', 'UT (Unique ID)', 'Pubmed Id', 'Author full names', 'Author(s) ID', 'Year', 'Source title', 'Art. No.', 'Page start', 'Page end', 'Page count', 'Cited by', 'Link', 'Publication Stage', 'Open Access', 'Source', 'EID']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Possible columns that may contain country / affiliation info\n",
        "possible_country_cols = [\n",
        "    \"Affiliation Country\",\n",
        "    \"Affiliations\",\n",
        "    \"Author Affiliation\",\n",
        "    \"Author Affiliations\",\n",
        "    \"Affiliations 1\",\n",
        "    \"Country\",\n",
        "    \"Affiliation\"\n",
        "]\n",
        "\n",
        "# Keep only the ones that actually exist in your dataframe\n",
        "country_cols = [c for c in possible_country_cols if c in deduped_df.columns]\n",
        "\n",
        "print(\"Using these columns for country detection:\")\n",
        "print(country_cols)\n",
        "\n",
        "if not country_cols:\n",
        "    raise ValueError(\"No country/affiliation columns found. Please update 'possible_country_cols' list.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qe4HANQf8SH",
        "outputId": "271fd0c2-7f4c-4618-dd88-b69333403435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using these columns for country detection:\n",
            "['Author Affiliations']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_countries = [\n",
        "    \"Pakistan\",\n",
        "    \"Bangladesh\",\n",
        "    \"Nepal\",\n",
        "    \"Sri Lanka\",\n",
        "    \"Bhutan\",\n",
        "    \"Maldives\",\n",
        "    \"India\",\n",
        "    \"Afghanistan\",\n",
        "    \"South Asia\",   # in case region appears\n",
        "    \"South Asian\"\n",
        "]"
      ],
      "metadata": {
        "id": "QocXusEAgDC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_countries_in_row(row):\n",
        "    text = \"\"\n",
        "    for col in country_cols:\n",
        "        text += \" \" + str(row[col])\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    found = []\n",
        "    for country in target_countries:\n",
        "        if country.lower() in text_lower:\n",
        "            found.append(country)\n",
        "    # return unique list\n",
        "    return list(set(found))\n",
        "\n",
        "# Apply detection\n",
        "deduped_df[\"countries_detected\"] = deduped_df.apply(find_countries_in_row, axis=1)\n",
        "\n",
        "# Explode list of countries into long format\n",
        "exploded = deduped_df.explode(\"countries_detected\")\n",
        "\n",
        "# Remove rows with no detected country\n",
        "exploded = exploded[exploded[\"countries_detected\"].notna() & (exploded[\"countries_detected\"] != \"\")]\n",
        "\n",
        "# Count papers per country (by unique title_clean if available, else by index)\n",
        "paper_id_col = \"title_clean\" if \"title_clean\" in deduped_df.columns else None\n",
        "\n",
        "if paper_id_col:\n",
        "    country_counts = exploded.groupby(\"countries_detected\")[paper_id_col].nunique().reset_index()\n",
        "else:\n",
        "    country_counts = exploded[\"countries_detected\"].value_counts().reset_index()\n",
        "    country_counts.columns = [\"countries_detected\", \"count\"]\n",
        "\n",
        "country_counts = country_counts.sort_values(by=country_counts.columns[-1], ascending=False)\n",
        "\n",
        "print(\"\\nðŸ“Š Papers per country:\")\n",
        "print(country_counts)\n",
        "\n",
        "# Save to CSV\n",
        "country_counts.to_csv(\"/content/papers_per_country.csv\", index=False)\n",
        "print(\"\\nSaved summary to /content/papers_per_country.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqfsXPbFgHIN",
        "outputId": "f7745caf-fa03-4fca-a224-6301a43756a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š Papers per country:\n",
            "  countries_detected  title_clean\n",
            "0         Bangladesh           27\n",
            "1           Pakistan           15\n",
            "2          Sri Lanka            6\n",
            "\n",
            "Saved summary to /content/papers_per_country.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zwqrVg4GgJsI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}