{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdde4d21-760b-4cc1-93d5-766d7dceb73f",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1:\n",
    "A- Load the provided CSV dataset (sample-superstore.csv) into Python and print the first ten records with the associated column names.\n",
    "\n",
    "B- Provide a short paragraph to describe your understanding of the dataset. (around 100 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107961d-932f-47d6-af5f-6a657312f5f2",
   "metadata": {},
   "source": [
    "### Task 1 - Part A: Load and Explore the Dataset\n",
    "In this task, we’ll use the Pandas library to load the sample-superstore.csv dataset and define two reusable methods:\n",
    "- head(limit) – Returns the top N rows of the dataset.\n",
    "- tail(limit) – Returns the bottom N rows of the dataset.\n",
    "\n",
    "These methods take a parameter limit that specifies how many rows to return.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11902da1-3c41-4fce-91b2-5cc50f248b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   Row ID | Order ID       | Order Date   | Ship Date   | Ship Mode      | Customer ID   | Customer Name    | Segment     | Country       | City         | State          | Postal Code   | Region   | Product ID      | Category        | Sub-Category   | Product Name                                                                |    Sales | Quantity   |   Discount | Profit       |\n",
      "|---:|---------:|:---------------|:-------------|:------------|:---------------|:--------------|:-----------------|:------------|:--------------|:-------------|:---------------|:--------------|:---------|:----------------|:----------------|:---------------|:----------------------------------------------------------------------------|---------:|:-----------|-----------:|:-------------|\n",
      "|  0 |     7773 | CA-2016-108196 | 25/11/2016   | 12/02/2016  | Standard Class | CS-12505      | Cindy Stewart    | Consumer    | United States | Lancaster    | Ohio           | 43130         | Est      | TEC-MA-10000418 | Technology      | Machines       | Cubify CubeX 3D Printer Double Head Print                                   |  4499.98 | 5          |        0.7 | -6599.978    |\n",
      "|  1 |      684 | US-2017-168116 | 11/04/2017   | 11/04/2017  | Same Day       | GT-14635      | Grant Thornton   | Corporate   | United States | Burlington   | North Carolina | \"27217\"       | South    | TEC-MA-10004125 | Technology      | Machines       | Cubify CubeX 3D Printer Triple Head Print                                   |  7999.98 | 4          |        0.5 | -3839.9904   |\n",
      "|  2 |     9775 | CA-2014-169019 | 26/07/2014   | 30/07/2014  | Standard Class | LF-17185      | Luke Foster      | Consumer    | United States | San Antonio  | Texas          | 78207         | Central  | OFF-BI-10004995 | Office Supplies | Binders        | GBC DocuBind P400 Electric Binding System                                   |  2177.58 | 8          |        0.8 | -3701.8928   |\n",
      "|  3 |     3012 | CA-2017-134845 | 17/04/2017   | 24/04/2017  | Standard Class | SR-20425      | Sharelle Roach   | Home Office | United States | Louisville   | Colorado       | 80027         | West     | TEC-MA-10000822 | Technology      | Machines       | Lexmark MX611dhe Monochrome Laser Printer                                   |  2549.99 | 5          |        0.7 | -3399.98     |\n",
      "|  4 |     4992 | US-2017-122714 | 12/07/2017   | 13/12/2017  | Standard Class | HG-14965      | Henry Goldwyn    | Corporate   | United States | Chicago      | Illinois       | 60653         | Central  | OFF-BI-10001120 | Office Supplies | Binders        | Ibico EPK-21 Electric Binding System                                        |  1889.99 | 5          |        0.8 | -2929.4845   |\n",
      "|  5 |     3152 | CA-2015-147830 | 15/12/2015   | 18/12/2015  | First Class    | NF-18385      | Natalie Fritzler | Consumer    | United States | Newark       | Ohio           | 43055         | East     | TEC-MA-10000418 | Technology      | Machines       | Cubify CubeX 3D Printer Double Head Print                                   |  1799.99 | Two        |        0.7 | \"-2639.9912\" |\n",
      "|  6 |     5311 | CA-2017-131254 | 19/11/2017   | 21/11/2017  | First Class    | NC-18415      | Nathan Cano      | Consumer    | United States | Houston      | Texas          | 77095         | Central  | OFF-BI-10003527 | Office Supplies | Binders        | Fellowes PB500 Electric Punch Plastic Comb Binding Machine with Manual Bind |  1525.19 | 6          |        0.8 | -2287.782    |\n",
      "|  7 |     9640 | CA-2015-116638 | 28/01/2015   | nan         | Second Class   | JH-15985      | Joseph Holt      | Consumer    | United States | Concord      | North Carolina | 28027         | South    | FUR-TA-10000198 | Frnture         | Tables         | Chromcraft Bull-Nose Wood Oval Conference Tables & Bases                    |  4297.64 | Thirteen   |        0.4 | nan          |\n",
      "|  8 |     1200 | CA-2016-130946 | 04/08/2016   | 04/12/2016  | Standard Class | ZC-21910      | Zuschuss Carroll | Consumer    | United States | Houston      | Texas          | 77041         | Central  | OFF-BI-10004995 | Office Supplies | Binders        | GBC DocuBind P400 Electric Binding System                                   |  1088.79 | 4          |        0.8 | -1850.9464   |\n",
      "|  9 |     2698 | CA-2014-145317 | 18/03/2014   | 23/03/2014  | Standard Class | SM-20320      | Sean Miller      | Home Office | nan           | Jacksonville | Florida        | 32216         | Southh   | TEC-MA-10002412 | Technology      | Machines       | Cisco TelePresence System EX90 Videoconferencing Unit                       | 22638.5  | 6          |        0.5 | -1811.0784   |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "class ExploratoryDataAnalysis:\n",
    "    ## Class-level variables\n",
    "    store_data_frame: DataFrame = None\n",
    "    \n",
    "    def __init__(self, path: str):\n",
    "        self.store_data_frame = pd.read_csv(path, sep=',')\n",
    "\n",
    "    def head(self, limit: int):\n",
    "        return self.store_data_frame.head(limit)\n",
    "\n",
    "    def tail(self, limit: int):\n",
    "        return self.store_data_frame.tail(limit)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    exp_data_analysis = ExploratoryDataAnalysis('sample-superstore.csv')\n",
    "    ## printing first 10 records associated with column names\n",
    "    print(exp_data_analysis.head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75d86f-411b-404b-ac90-53a819fd8a68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### Task 1 Part B - Understanding the Sample Superstore Dataset\n",
    "\n",
    "The Sample Superstore dataset captures detailed retail sales data from a fictional store. It includes information about customer orders such as \n",
    "- `Order ID`\n",
    "- `Order Date`\n",
    "- `Ship Mode`\n",
    "- `Customer Name`\n",
    "- `Segment`\n",
    "- `City`\n",
    "- `State`\n",
    "- `Region`\n",
    "\n",
    "Each transaction is linked to a product with,\n",
    "- `Product ID`\n",
    "- `Category`\n",
    "- `Sub-Category`\n",
    "- `Product Name`\n",
    "\n",
    "Each transaction contains metrics such as \n",
    "- `sales`\n",
    "- `Quantity`\n",
    "- `Discount`\n",
    "- `Profit`\n",
    "\n",
    "This dataset is ideal for analysing customer purchasing behaviour, shipping performance, product profitability, and regional sales trends. It can be used in data science for performing exploratory data analysis (EDA), creating dashboards, and building predictive business models.\n",
    "\n",
    "### Loading Data\n",
    "To work with this data in Python, we use the Pandas library, which provides powerful tools for data manipulation and analysis. We load the dataset using `pd.read_csv()`, which reads the CSV file and returns a `DataFrame`. A DataFrame is a two-dimensional labelled data structure in Pandas, similar to a table in a database or an Excel spreadsheet. It allows us to easily inspect, filter, sort, and transform the data. Once loaded, the dataset becomes a DataFrame object where each row represents a single order and each column represents a different attribute related to the order.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cabb8e5-b3b5-40a9-9a37-18bddf565ac3",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "Process the dataset's variables and conduct exploratory data analysis. Explore the dataset thoroughly, and feel free to improvise as needed. However, you must use Python for at least four of the following techniques:\n",
    "\n",
    "### Task 2 - Part A: Descriptive statistics\n",
    "In this task, we will perform Exploratory Data Analysis (EDA) on the **Sample Superstore** dataset using Python. The goal is to better understand the structure, quality, and patterns in the data before moving on to visualisation or modelling.\n",
    "\n",
    "We will cover the following key techniques as part of our EDA:\n",
    "\n",
    "1. **Descriptive Statistics**  \n",
    "   Generate summary statistics for numerical columns (e.g., Sales, Profit, Discount).\n",
    "\n",
    "2. **Data Type Inspection**  \n",
    "   Understand the data structure and identify numerical vs. categorical features.\n",
    "\n",
    "3. **Missing Value Analysis**  \n",
    "   Detect any null or missing values that might require cleaning.\n",
    "\n",
    "4. **Unique Value Counts and Categorical Distributions**  \n",
    "   Explore unique values in categorical columns (e.g., Segment, Category, Region).\n",
    "\n",
    "5. **Aggregated Insights**  \n",
    "   Analyse performance (e.g., sales and profit) across different categories and regions.\n",
    "\n",
    "This initial analysis will help us identify trends, detect anomalies, and prepare the dataset for deeper visualisation or machine learning tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0b93733-067d-4210-8dd0-1c6734c18af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Dataset:  (9994, 21)\n",
      "\n",
      "Column Names:  ['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Quantity', 'Discount', 'Profit']\n",
      "\n",
      "Descriptive Statistics:              Row ID         Sales     Discount\n",
      "count  9994.000000   9993.000000  9991.000000\n",
      "mean   4997.500000    229.863780     0.156180\n",
      "std    2885.163629    623.276019     0.206399\n",
      "min       1.000000      0.444000     0.000000\n",
      "25%    2499.250000     17.280000     0.000000\n",
      "50%    4997.500000     54.480000     0.200000\n",
      "75%    7495.750000    209.940000     0.200000\n",
      "max    9994.000000  22638.480000     0.800000\n",
      "\n",
      "Data types and non-null counts: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9994 entries, 0 to 9993\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Row ID         9994 non-null   int64  \n",
      " 1   Order ID       9993 non-null   object \n",
      " 2   Order Date     9992 non-null   object \n",
      " 3   Ship Date      9991 non-null   object \n",
      " 4   Ship Mode      9990 non-null   object \n",
      " 5   Customer ID    9994 non-null   object \n",
      " 6   Customer Name  9991 non-null   object \n",
      " 7   Segment        9991 non-null   object \n",
      " 8   Country        9990 non-null   object \n",
      " 9   City           9992 non-null   object \n",
      " 10  State          9990 non-null   object \n",
      " 11  Postal Code    9991 non-null   object \n",
      " 12  Region         9991 non-null   object \n",
      " 13  Product ID     9992 non-null   object \n",
      " 14  Category       9992 non-null   object \n",
      " 15  Sub-Category   9990 non-null   object \n",
      " 16  Product Name   9991 non-null   object \n",
      " 17  Sales          9993 non-null   float64\n",
      " 18  Quantity       9989 non-null   object \n",
      " 19  Discount       9991 non-null   float64\n",
      " 20  Profit         9983 non-null   object \n",
      "dtypes: float64(2), int64(1), object(18)\n",
      "memory usage: 1.6+ MB\n",
      "None\n",
      "\n",
      "Missing values in each column: \n",
      "Row ID            0\n",
      "Order ID          1\n",
      "Order Date        2\n",
      "Ship Date         3\n",
      "Ship Mode         4\n",
      "Customer ID       0\n",
      "Customer Name     3\n",
      "Segment           3\n",
      "Country           4\n",
      "City              2\n",
      "State             4\n",
      "Postal Code       3\n",
      "Region            3\n",
      "Product ID        2\n",
      "Category          2\n",
      "Sub-Category      4\n",
      "Product Name      3\n",
      "Sales             1\n",
      "Quantity          5\n",
      "Discount          3\n",
      "Profit           11\n",
      "dtype: int64\n",
      "\n",
      "Columns with ≤ 5 unique values (possible categorical features):\n",
      "\n",
      "['Ship Mode', 'Segment', 'Country', 'Category']\n",
      "\n",
      "Filling up missing values in categorical columns with value 'UNKNOWN'\n",
      "\n",
      "\n",
      "Column Ship Mode have values ['Standard Class' 'Same Day' 'First Class' 'Second Class' 'UNKNOWN'].\n",
      "Column Segment have values ['Consumer' 'Corporate' 'Home Office' '%' 'UNKNOWN'].\n",
      "Column Country have values ['United States' 'UNKNOWN' 'US' '56'].\n",
      "Column Category have values ['Technology' 'Office Supplies' 'Frnture' 'Furniture' 'UNKNOWN'].\n",
      "\n",
      "\n",
      "Replaced 1 occurrence(s) of '%' with 'UNKNOWN' in column 'Segment'.\n",
      "Total now: 4 instance(s) of 'UNKNOWN' in 'Segment'.\n",
      "\n",
      "\n",
      "Replaced 1 occurrence(s) of '56' with 'UNKNOWN' in column 'Country'.\n",
      "Total now: 5 instance(s) of 'UNKNOWN' in 'Country'.\n",
      "\n",
      "\n",
      "Column Ship Mode have values ['Standard Class' 'Same Day' 'First Class' 'Second Class' 'UNKNOWN'].\n",
      "Column Segment have values ['Consumer' 'Corporate' 'Home Office' 'UNKNOWN'].\n",
      "Column Country have values ['United States' 'UNKNOWN' 'US'].\n",
      "Column Category have values ['Technology' 'Office Supplies' 'Frnture' 'Furniture' 'UNKNOWN'].\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "class ColumnNotFoundError(Exception):\n",
    "    def __init__(self, missing_columns):\n",
    "        message = f\"The following column(s) are missing: {missing_columns}\"\n",
    "        super().__init__(message)\n",
    "        self.missing_columns = missing_columns\n",
    "\n",
    "class ExploratoryDataAnalysis:\n",
    "    ## Class-level variables\n",
    "    store_data_frame: DataFrame = None\n",
    "    \n",
    "    def __init__(self, path: str):\n",
    "        self.store_data_frame = pd.read_csv(path, sep=',')\n",
    "\n",
    "    def inspect(self):\n",
    "        print(\"Shape of Dataset: \", self.store_data_frame.shape)\n",
    "        print(\"\\nColumn Names: \", self.store_data_frame.columns.tolist())\n",
    "\n",
    "    def descriptive_stats(self):\n",
    "        print(\"\\nDescriptive Statistics: \", self.store_data_frame.describe())\n",
    "\n",
    "    def basic_info(self):\n",
    "        print(\"\\nData types and non-null counts: \")\n",
    "        print(self.store_data_frame.info())\n",
    "\n",
    "    def missing_value_info(self):\n",
    "        print(\"\\nMissing values in each column: \")\n",
    "        print(self.store_data_frame.isnull().sum())\n",
    "\n",
    "    def get_categorical_candidates(self, threshold=10):\n",
    "        \"\"\"\n",
    "        Return columns that have unique values less than or equal to `threshold`.\n",
    "        These are good candidates for categorical analysis.\n",
    "        \"\"\"\n",
    "        candidate_columns = []\n",
    "        \n",
    "        print(f\"\\nColumns with ≤ {threshold} unique values (possible categorical features):\\n\")\n",
    "        for col in self.store_data_frame.columns:\n",
    "            unique_values = self.store_data_frame[col].nunique()\n",
    "            if unique_values <= threshold:\n",
    "                candidate_columns.append((col, unique_values))\n",
    "\n",
    "        return [col for col, _ in candidate_columns]\n",
    "\n",
    "    def fill_missing_values(self, categorical_columns):\n",
    "        \"\"\"\n",
    "        This method will fill all missing values for categorical columns with UNKNOWN.\n",
    "        \"\"\"\n",
    "        missing = [col for col in categorical_columns if col not in self.store_data_frame.columns]\n",
    "        if missing:\n",
    "            raise ColumnNotFoundError(missing)\n",
    "            \n",
    "        print(\"\\nFilling up missing values in categorical columns with value 'UNKNOWN'\")\n",
    "        for col in categorical_columns:\n",
    "            self.store_data_frame[col] = self.store_data_frame[col].fillna(\"UNKNOWN\")\n",
    "\n",
    "    def print_uniques(self, columns = None):\n",
    "        missing = [col for col in categorical_columns if col not in self.store_data_frame.columns]\n",
    "        if missing:\n",
    "            raise ColumnNotFoundError(missing)\n",
    "         \n",
    "        print(\"\\n\")   \n",
    "        if columns:\n",
    "            for col in columns:\n",
    "                print(f\"Column {col} have values {self.store_data_frame[col].unique()}.\")\n",
    "        else:\n",
    "            print(self.store_data_frame.head(5))\n",
    "\n",
    "    def replace_values(self, column, currVal, newVal):\n",
    "        if column not in self.store_data_frame.columns:\n",
    "            raise ColumnNotFoundError(column)\n",
    "\n",
    "        print(\"\\n\") \n",
    "        ## Count how many current values will be replaced\n",
    "        count_before = (self.store_data_frame[column] == currVal).sum()\n",
    "        ## Perform Replacement\n",
    "        self.store_data_frame[column] = self.store_data_frame[column].replace(currVal, newVal)\n",
    "        ## Count how many new values being replaced\n",
    "        count_after = (self.store_data_frame[column] == newVal).sum()\n",
    "\n",
    "        print(f\"Replaced {count_before} occurrence(s) of '{currVal}' with '{newVal}' in column '{column}'.\")\n",
    "        print(f\"Total now: {count_after} instance(s) of '{newVal}' in '{column}'.\")\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        exp_data_analysis = ExploratoryDataAnalysis('sample-superstore.csv')\n",
    "        ## Step 1: Load and Inspect the Dataset\n",
    "        exp_data_analysis.inspect()\n",
    "        ## Step 2: Data Types and Basic Info\n",
    "        exp_data_analysis.basic_info()\n",
    "        ## Step 3: Null / Missing Value Analysis\n",
    "        exp_data_analysis.missing_value_info()\n",
    "        ## Step 4: Categorical Analysis - This will return ['Ship Mode', 'Segment', 'Country', 'Category']\n",
    "        categorical_columns = exp_data_analysis.get_categorical_candidates(5)\n",
    "        print(categorical_columns)\n",
    "        ## Step 5: Fill Missing Values for Categorical Columns\n",
    "        exp_data_analysis.fill_missing_values(categorical_columns)\n",
    "        ## Step 6: Print Aftering Filling Missing Values\n",
    "        exp_data_analysis.print_uniques(categorical_columns)\n",
    "        ## Step 7: Clean Data\n",
    "        ## - As Segment have % values let's replace those with UNKNOWN\n",
    "        exp_data_analysis.replace_values('Segment', '%', 'UNKNOWN')\n",
    "        ## - As Country have 56 values let's replace those with UNKNOWN\n",
    "        exp_data_analysis.replace_values('Country', '56', 'UNKNOWN')\n",
    "        ## Step 8: Print Aftering Replacing Bad Values\n",
    "        exp_data_analysis.print_uniques(categorical_columns)\n",
    "        ## Step 9: Descriptive Statistics\n",
    "        exp_data_analysis.descriptive_stats()\n",
    "    except ColumnNotFoundError as e:    \n",
    "        print(f\"\\nColumnNotFoundError : {e}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
