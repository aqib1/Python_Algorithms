{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdde4d21-760b-4cc1-93d5-766d7dceb73f",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1:\n",
    "A- Load the provided CSV dataset (sample-superstore.csv) into Python and print the first ten records with the associated column names.\n",
    "\n",
    "B- Provide a short paragraph to describe your understanding of the dataset. (around 100 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107961d-932f-47d6-af5f-6a657312f5f2",
   "metadata": {},
   "source": [
    "### Task 1 - Part A: Load and Explore the Dataset\n",
    "In this task, we’ll use the Pandas library to load the sample-superstore.csv dataset and define two reusable methods:\n",
    "- head(limit) – Returns the top N rows of the dataset.\n",
    "- tail(limit) – Returns the bottom N rows of the dataset.\n",
    "\n",
    "These methods take a parameter limit that specifies how many rows to return.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11902da1-3c41-4fce-91b2-5cc50f248b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   Row ID | Order ID       | Order Date   | Ship Date   | Ship Mode      | Customer ID   | Customer Name    | Segment     | Country       | City         | State          | Postal Code   | Region   | Product ID      | Category        | Sub-Category   | Product Name                                                                |    Sales | Quantity   |   Discount | Profit       |\n",
      "|---:|---------:|:---------------|:-------------|:------------|:---------------|:--------------|:-----------------|:------------|:--------------|:-------------|:---------------|:--------------|:---------|:----------------|:----------------|:---------------|:----------------------------------------------------------------------------|---------:|:-----------|-----------:|:-------------|\n",
      "|  0 |     7773 | CA-2016-108196 | 25/11/2016   | 12/02/2016  | Standard Class | CS-12505      | Cindy Stewart    | Consumer    | United States | Lancaster    | Ohio           | 43130         | Est      | TEC-MA-10000418 | Technology      | Machines       | Cubify CubeX 3D Printer Double Head Print                                   |  4499.98 | 5          |        0.7 | -6599.978    |\n",
      "|  1 |      684 | US-2017-168116 | 11/04/2017   | 11/04/2017  | Same Day       | GT-14635      | Grant Thornton   | Corporate   | United States | Burlington   | North Carolina | \"27217\"       | South    | TEC-MA-10004125 | Technology      | Machines       | Cubify CubeX 3D Printer Triple Head Print                                   |  7999.98 | 4          |        0.5 | -3839.9904   |\n",
      "|  2 |     9775 | CA-2014-169019 | 26/07/2014   | 30/07/2014  | Standard Class | LF-17185      | Luke Foster      | Consumer    | United States | San Antonio  | Texas          | 78207         | Central  | OFF-BI-10004995 | Office Supplies | Binders        | GBC DocuBind P400 Electric Binding System                                   |  2177.58 | 8          |        0.8 | -3701.8928   |\n",
      "|  3 |     3012 | CA-2017-134845 | 17/04/2017   | 24/04/2017  | Standard Class | SR-20425      | Sharelle Roach   | Home Office | United States | Louisville   | Colorado       | 80027         | West     | TEC-MA-10000822 | Technology      | Machines       | Lexmark MX611dhe Monochrome Laser Printer                                   |  2549.99 | 5          |        0.7 | -3399.98     |\n",
      "|  4 |     4992 | US-2017-122714 | 12/07/2017   | 13/12/2017  | Standard Class | HG-14965      | Henry Goldwyn    | Corporate   | United States | Chicago      | Illinois       | 60653         | Central  | OFF-BI-10001120 | Office Supplies | Binders        | Ibico EPK-21 Electric Binding System                                        |  1889.99 | 5          |        0.8 | -2929.4845   |\n",
      "|  5 |     3152 | CA-2015-147830 | 15/12/2015   | 18/12/2015  | First Class    | NF-18385      | Natalie Fritzler | Consumer    | United States | Newark       | Ohio           | 43055         | East     | TEC-MA-10000418 | Technology      | Machines       | Cubify CubeX 3D Printer Double Head Print                                   |  1799.99 | Two        |        0.7 | \"-2639.9912\" |\n",
      "|  6 |     5311 | CA-2017-131254 | 19/11/2017   | 21/11/2017  | First Class    | NC-18415      | Nathan Cano      | Consumer    | United States | Houston      | Texas          | 77095         | Central  | OFF-BI-10003527 | Office Supplies | Binders        | Fellowes PB500 Electric Punch Plastic Comb Binding Machine with Manual Bind |  1525.19 | 6          |        0.8 | -2287.782    |\n",
      "|  7 |     9640 | CA-2015-116638 | 28/01/2015   | nan         | Second Class   | JH-15985      | Joseph Holt      | Consumer    | United States | Concord      | North Carolina | 28027         | South    | FUR-TA-10000198 | Frnture         | Tables         | Chromcraft Bull-Nose Wood Oval Conference Tables & Bases                    |  4297.64 | Thirteen   |        0.4 | nan          |\n",
      "|  8 |     1200 | CA-2016-130946 | 04/08/2016   | 04/12/2016  | Standard Class | ZC-21910      | Zuschuss Carroll | Consumer    | United States | Houston      | Texas          | 77041         | Central  | OFF-BI-10004995 | Office Supplies | Binders        | GBC DocuBind P400 Electric Binding System                                   |  1088.79 | 4          |        0.8 | -1850.9464   |\n",
      "|  9 |     2698 | CA-2014-145317 | 18/03/2014   | 23/03/2014  | Standard Class | SM-20320      | Sean Miller      | Home Office | nan           | Jacksonville | Florida        | 32216         | Southh   | TEC-MA-10002412 | Technology      | Machines       | Cisco TelePresence System EX90 Videoconferencing Unit                       | 22638.5  | 6          |        0.5 | -1811.0784   |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "class ExploratoryDataAnalysis:\n",
    "    ## Class-level variables\n",
    "    store_data_frame: DataFrame = None\n",
    "    \n",
    "    def __init__(self, path: str):\n",
    "        self.store_data_frame = pd.read_csv(path, sep=',')\n",
    "\n",
    "    def head(self, limit: int):\n",
    "        return self.store_data_frame.head(limit)\n",
    "\n",
    "    def tail(self, limit: int):\n",
    "        return self.store_data_frame.tail(limit)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    exp_data_analysis = ExploratoryDataAnalysis('sample-superstore.csv')\n",
    "    ## printing first 10 records associated with column names\n",
    "    print(exp_data_analysis.head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75d86f-411b-404b-ac90-53a819fd8a68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### Task 1 Part B - Understanding the Sample Superstore Dataset\n",
    "\n",
    "The Sample Superstore dataset captures detailed retail sales data from a fictional store. It includes information about customer orders such as \n",
    "- `Order ID`\n",
    "- `Order Date`\n",
    "- `Ship Mode`\n",
    "- `Customer Name`\n",
    "- `Segment`\n",
    "- `City`\n",
    "- `State`\n",
    "- `Region`\n",
    "\n",
    "Each transaction is linked to a product with,\n",
    "- `Product ID`\n",
    "- `Category`\n",
    "- `Sub-Category`\n",
    "- `Product Name`\n",
    "\n",
    "Each transaction contains metrics such as \n",
    "- `sales`\n",
    "- `Quantity`\n",
    "- `Discount`\n",
    "- `Profit`\n",
    "\n",
    "This dataset is ideal for analysing customer purchasing behaviour, shipping performance, product profitability, and regional sales trends. It can be used in data science for performing exploratory data analysis (EDA), creating dashboards, and building predictive business models.\n",
    "\n",
    "### Loading Data\n",
    "To work with this data in Python, we use the Pandas library, which provides powerful tools for data manipulation and analysis. We load the dataset using `pd.read_csv()`, which reads the CSV file and returns a `DataFrame`. A DataFrame is a two-dimensional labelled data structure in Pandas, similar to a table in a database or an Excel spreadsheet. It allows us to easily inspect, filter, sort, and transform the data. Once loaded, the dataset becomes a DataFrame object where each row represents a single order and each column represents a different attribute related to the order.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cabb8e5-b3b5-40a9-9a37-18bddf565ac3",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "Process the dataset's variables and conduct exploratory data analysis. Explore the dataset thoroughly, and feel free to improvise as needed. However, you must use Python for at least four of the following techniques:\n",
    "\n",
    "- Descriptive statistics: Describe features of the data set by generating summaries about data samples.\n",
    "- Outlier treatment: Identify abnormal or problematic values and apply methods to treat them.\n",
    "- Normalising and scaling (numerical variables): Apply normalisation and scaling methods to transform data for further analysis.\n",
    "- Grouping of data: Demonstrate data aggregations or frequency distributions to summarise analysis.\n",
    "- Handling missing values in the dataset: Identify methods for cleaning the dataset.\n",
    "- Correlation: Describe features that are related and the nature of that relationship.\n",
    "- Univariate analysis and visualisation: Use different visualisation methods for demonstrating your analysis scenarios.\n",
    "\n",
    "### Task 2 - Part A: Descriptive statistics\n",
    "In this task, we will perform Exploratory Data Analysis (EDA) on the **Sample Superstore** dataset using Python. The goal is to better understand the structure, quality, and patterns in the data before moving on to visualisation or modelling.\n",
    "\n",
    "We will cover the following key techniques as part of our EDA:\n",
    "\n",
    "1. **Descriptive Statistics**  \n",
    "   Generate summary statistics for numerical columns (e.g., Sales, Profit, Discount).\n",
    "\n",
    "2. **Data Type Inspection**  \n",
    "   Understand the data structure and identify numerical vs. categorical features.\n",
    "\n",
    "3. **Missing Value Analysis**  \n",
    "   Detect any null or missing values that might require cleaning.\n",
    "\n",
    "4. **Unique Value Counts and Categorical Distributions**  \n",
    "   Explore unique values in categorical columns (e.g., Segment, Category, Region).\n",
    "\n",
    "5. **Aggregated Insights**  \n",
    "   Analyse performance (e.g., sales and profit) across different categories and regions.\n",
    "\n",
    "This initial analysis will help us identify trends, detect anomalies, and prepare the dataset for deeper visualisation or machine learning tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b93733-067d-4210-8dd0-1c6734c18af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Dataset:  (9994, 21)\n",
      "\n",
      "Column Names:  ['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Quantity', 'Discount', 'Profit']\n",
      "\n",
      "Data types and non-null counts: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9994 entries, 0 to 9993\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Row ID         9994 non-null   int64  \n",
      " 1   Order ID       9993 non-null   object \n",
      " 2   Order Date     9992 non-null   object \n",
      " 3   Ship Date      9991 non-null   object \n",
      " 4   Ship Mode      9990 non-null   object \n",
      " 5   Customer ID    9994 non-null   object \n",
      " 6   Customer Name  9991 non-null   object \n",
      " 7   Segment        9991 non-null   object \n",
      " 8   Country        9990 non-null   object \n",
      " 9   City           9992 non-null   object \n",
      " 10  State          9990 non-null   object \n",
      " 11  Postal Code    9991 non-null   object \n",
      " 12  Region         9991 non-null   object \n",
      " 13  Product ID     9992 non-null   object \n",
      " 14  Category       9992 non-null   object \n",
      " 15  Sub-Category   9990 non-null   object \n",
      " 16  Product Name   9991 non-null   object \n",
      " 17  Sales          9993 non-null   float64\n",
      " 18  Quantity       9989 non-null   object \n",
      " 19  Discount       9991 non-null   float64\n",
      " 20  Profit         9983 non-null   object \n",
      "dtypes: float64(2), int64(1), object(18)\n",
      "memory usage: 1.6+ MB\n",
      "None\n",
      "\n",
      "Missing values in each column: \n",
      "Row ID            0\n",
      "Order ID          1\n",
      "Order Date        2\n",
      "Ship Date         3\n",
      "Ship Mode         4\n",
      "Customer ID       0\n",
      "Customer Name     3\n",
      "Segment           3\n",
      "Country           4\n",
      "City              2\n",
      "State             4\n",
      "Postal Code       3\n",
      "Region            3\n",
      "Product ID        2\n",
      "Category          2\n",
      "Sub-Category      4\n",
      "Product Name      3\n",
      "Sales             1\n",
      "Quantity          5\n",
      "Discount          3\n",
      "Profit           11\n",
      "dtype: int64\n",
      "\n",
      "Columns with ≤ 5 unique values (possible categorical features):\n",
      "\n",
      "['Ship Mode', 'Segment', 'Country', 'Category']\n",
      "\n",
      "Filling up missing values in categorical columns with value 'UNKNOWN'\n",
      "\n",
      "\n",
      "Column Ship Mode have values ['Standard Class' 'Same Day' 'First Class' 'Second Class' 'UNKNOWN'].\n",
      "Column Segment have values ['Consumer' 'Corporate' 'Home Office' '%' 'UNKNOWN'].\n",
      "Column Country have values ['United States' 'UNKNOWN' 'US' '56'].\n",
      "Column Category have values ['Technology' 'Office Supplies' 'Frnture' 'Furniture' 'UNKNOWN'].\n",
      "\n",
      "\n",
      "Replaced 1 occurrence(s) of '%' with 'UNKNOWN' in column 'Segment'.\n",
      "Total now: 4 instance(s) of 'UNKNOWN' in 'Segment'.\n",
      "\n",
      "\n",
      "Replaced 1 occurrence(s) of '56' with 'UNKNOWN' in column 'Country'.\n",
      "Total now: 5 instance(s) of 'UNKNOWN' in 'Country'.\n",
      "\n",
      "\n",
      "Replaced 1 occurrence(s) of 'Two' with '2' in column 'Quantity'.\n",
      "Total now: 1 instance(s) of '2' in 'Quantity'.\n",
      "\n",
      "\n",
      "Replaced 1 occurrence(s) of 'Thirteen' with '13' in column 'Quantity'.\n",
      "Total now: 1 instance(s) of '13' in 'Quantity'.\n",
      "\n",
      "\n",
      "Replaced 1 occurrence(s) of 'Seven' with '7' in column 'Quantity'.\n",
      "Total now: 1 instance(s) of '7' in 'Quantity'.\n",
      "\n",
      "\n",
      "Replaced 1 occurrence(s) of 'ten' with '10' in column 'Quantity'.\n",
      "Total now: 1 instance(s) of '10' in 'Quantity'.\n",
      "\n",
      "\n",
      "Replaced 1 occurrence(s) of '7?' with '7' in column 'Quantity'.\n",
      "Total now: 2 instance(s) of '7' in 'Quantity'.\n",
      "\n",
      "\n",
      "Column Ship Mode have values ['Standard Class' 'Same Day' 'First Class' 'Second Class' 'UNKNOWN'].\n",
      "Column Segment have values ['Consumer' 'Corporate' 'Home Office' 'UNKNOWN'].\n",
      "Column Country have values ['United States' 'UNKNOWN' 'US'].\n",
      "Column Category have values ['Technology' 'Office Supplies' 'Frnture' 'Furniture' 'UNKNOWN'].\n",
      "\n",
      "\n",
      "Column Ship Mode type is converted to category\n",
      "Column Segment type is converted to category\n",
      "Column Country type is converted to category\n",
      "Column Category type is converted to category\n",
      "\n",
      "\n",
      "Column Profit type is converted to float\n",
      "Column Quantity type is converted to float\n",
      "\n",
      "Descriptive Statistics:              Row ID         Sales     Quantity     Discount       Profit\n",
      "count  9994.000000   9993.000000  9989.000000  9991.000000  9983.000000\n",
      "mean   4997.500000    229.863780     3.789368     0.156180    29.080659\n",
      "std    2885.163629    623.276019     2.225495     0.206399   233.222001\n",
      "min       1.000000      0.444000     1.000000     0.000000 -6599.978000\n",
      "25%    2499.250000     17.280000     2.000000     0.000000     1.744300\n",
      "50%    4997.500000     54.480000     3.000000     0.200000     8.674400\n",
      "75%    7495.750000    209.940000     5.000000     0.200000    29.368000\n",
      "max    9994.000000  22638.480000    14.000000     0.800000  8399.976000\n",
      "\n",
      "Sales and Profit by Category:                        Sales       Profit\n",
      "Category                                 \n",
      "Technology       836154.0330  145482.6639\n",
      "Furniture        733206.6573   22687.3730\n",
      "Office Supplies  718808.1040  123953.9599\n",
      "Frnture            8621.0280   -1741.6017\n",
      "UNKNOWN             238.9280     -70.1760\n",
      "\n",
      "Profit by Region:               Profit\n",
      "Region              \n",
      "West     108412.6664\n",
      "East      99980.4204\n",
      "South     53684.4546\n",
      "Central   42093.2766\n",
      "Cntral     -471.8704\n",
      "Centrl    -1251.6690\n",
      "Southh    -3787.6115\n",
      "Est       -7988.5589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vk/p6qvytc90z38qct0h2zh9y8c0000gn/T/ipykernel_74590/4285027808.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  result = self.store_data_frame.groupby(group_by_col)[agg_cols].sum()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "class ColumnTypeConversionFailureException(Exception):\n",
    "    def __init__(self, message):\n",
    "        super().__init__(message)\n",
    "    \n",
    "class ColumnNotFoundError(Exception):\n",
    "    def __init__(self, missing_columns):\n",
    "        message = f\"The following column(s) are missing: {missing_columns}\"\n",
    "        super().__init__(message)\n",
    "        self.missing_columns = missing_columns\n",
    "\n",
    "class ExploratoryDataAnalysis:\n",
    "    ## Class-level variables\n",
    "    store_data_frame: DataFrame = None\n",
    "    \n",
    "    def __init__(self, path: str):\n",
    "        self.store_data_frame = pd.read_csv(path, sep=',')\n",
    "\n",
    "    def inspect(self):\n",
    "        print(\"Shape of Dataset: \", self.store_data_frame.shape)\n",
    "        print(\"\\nColumn Names: \", self.store_data_frame.columns.tolist())\n",
    "\n",
    "    def descriptive_stats(self):\n",
    "        print(\"\\nDescriptive Statistics: \", self.store_data_frame.describe())\n",
    "\n",
    "    def basic_info(self):\n",
    "        print(\"\\nData types and non-null counts: \")\n",
    "        print(self.store_data_frame.info())\n",
    "\n",
    "    def missing_value_info(self):\n",
    "        print(\"\\nMissing values in each column: \")\n",
    "        print(self.store_data_frame.isnull().sum())\n",
    "\n",
    "    def get_categorical_candidates(self, threshold=10):\n",
    "        \"\"\"\n",
    "        Return columns that have unique values less than or equal to `threshold`.\n",
    "        These are good candidates for categorical analysis.\n",
    "        \"\"\"\n",
    "        candidate_columns = []\n",
    "        \n",
    "        print(f\"\\nColumns with ≤ {threshold} unique values (possible categorical features):\\n\")\n",
    "        for col in self.store_data_frame.columns:\n",
    "            unique_values = self.store_data_frame[col].nunique()\n",
    "            if unique_values <= threshold:\n",
    "                candidate_columns.append((col, unique_values))\n",
    "\n",
    "        return [col for col, _ in candidate_columns]\n",
    "\n",
    "    def fill_missing_values(self, categorical_columns, fill_value):\n",
    "        \"\"\"\n",
    "        This method will fill all missing values for categorical columns with provided fill_value.\n",
    "        \"\"\"\n",
    "        missing = [col for col in categorical_columns if col not in self.store_data_frame.columns]\n",
    "        if missing:\n",
    "            raise ColumnNotFoundError(missing)\n",
    "            \n",
    "        print(f\"\\nFilling up missing values in categorical columns with value '{fill_value}'\")\n",
    "        for col in categorical_columns:\n",
    "            self.store_data_frame[col] = self.store_data_frame[col].fillna(fill_value)\n",
    "\n",
    "    def print_uniques(self, columns = None):\n",
    "        missing = [col for col in categorical_columns if col not in self.store_data_frame.columns]\n",
    "        if missing:\n",
    "            raise ColumnNotFoundError(missing)\n",
    "         \n",
    "        print(\"\\n\")   \n",
    "        if columns:\n",
    "            for col in columns:\n",
    "                print(f\"Column {col} have values {self.store_data_frame[col].unique()}.\")\n",
    "        else:\n",
    "            print(self.store_data_frame.head(5))\n",
    "\n",
    "    def replace_values(self, column, currVal, newVal):\n",
    "        if column not in self.store_data_frame.columns:\n",
    "            raise ColumnNotFoundError(column)\n",
    "\n",
    "        print(\"\\n\") \n",
    "        ## Count how many current values will be replaced\n",
    "        count_before = (self.store_data_frame[column] == currVal).sum()\n",
    "        ## Perform Replacement\n",
    "        self.store_data_frame[column] = self.store_data_frame[column].replace(currVal, newVal)\n",
    "        ## Count how many new values being replaced\n",
    "        count_after = (self.store_data_frame[column] == newVal).sum()\n",
    "\n",
    "        print(f\"Replaced {count_before} occurrence(s) of '{currVal}' with '{newVal}' in column '{column}'.\")\n",
    "        print(f\"Total now: {count_after} instance(s) of '{newVal}' in '{column}'.\")\n",
    "\n",
    "    def convert_columns_dtype(self, dtype_map):\n",
    "        \"\"\"\n",
    "        Convert specified columns to desired data types.\n",
    "\n",
    "        Parameters:\n",
    "        - dtype_map (dict): A dictionary where keys are column names and values are target data types.\n",
    "                        Example: {'Segment': 'category', 'Sales': 'float'}\n",
    "        \"\"\"\n",
    "        print(\"\\n\")\n",
    "        for col, dtype in dtype_map.items():\n",
    "            if col not in self.store_data_frame.columns:\n",
    "                raise ColumnNotFoundError(col)\n",
    "            try:\n",
    "                self.store_data_frame[col] = (self.store_data_frame[col].astype(str).str.replace('\"', '', regex=False).str.strip())\n",
    "                self.store_data_frame[col] = self.store_data_frame[col].astype(dtype)\n",
    "                print(f\"Column {col} type is converted to {dtype}\")\n",
    "            except Exception as ex:\n",
    "                error_message = f\"Failed to convert column '{col}' to '{dtype}': {str(ex)}\"\n",
    "                raise ColumnTypeConversionFailureException(error_message)\n",
    "\n",
    "    def group_and_summarize(self, group_by_col, agg_cols, sort_by=None, ascending=False):\n",
    "        \"\"\"\n",
    "        Groups a DataFrame by one or more columns and summarizes given numeric columns.\n",
    "\n",
    "        Parameters:\n",
    "            df (DataFrame): The input DataFrame.\n",
    "            group_by_col (str or list): Column(s) to group by.\n",
    "            agg_cols (str or list): Column(s) to aggregate (e.g., 'Sales', 'Profit').\n",
    "            sort_by (str): Column to sort the result by (must be in agg_cols).\n",
    "            ascending (bool): Sort order. Default is descending.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: Grouped and aggregated DataFrame.\n",
    "        \"\"\"\n",
    "        if isinstance(agg_cols, str):\n",
    "            agg_cols = [agg_cols]\n",
    "\n",
    "        result = self.store_data_frame.groupby(group_by_col)[agg_cols].sum()\n",
    "        if sort_by:\n",
    "            result = result.sort_values(by=sort_by, ascending=ascending)\n",
    "        return result\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        exp_data_analysis = ExploratoryDataAnalysis('sample-superstore.csv')\n",
    "        \n",
    "        # Step 1: Load and inspect the dataset\n",
    "        exp_data_analysis.inspect()\n",
    "        \n",
    "        # Step 2: Display data types and basic structure\n",
    "        exp_data_analysis.basic_info()\n",
    "        \n",
    "        # Step 3: Analyze missing values\n",
    "        exp_data_analysis.missing_value_info()\n",
    "        \n",
    "        # Step 4: Identify candidate categorical columns (based on unique value count)\n",
    "        categorical_columns = exp_data_analysis.get_categorical_candidates(5)\n",
    "        print(categorical_columns)\n",
    "        \n",
    "        # Step 5: Fill missing values in categorical columns with \"UNKNOWN\"\n",
    "        exp_data_analysis.fill_missing_values(categorical_columns, \"UNKNOWN\")\n",
    "        \n",
    "        # Step 6: Show unique values in categorical columns after filling\n",
    "        exp_data_analysis.print_uniques(categorical_columns)\n",
    "        \n",
    "        # Step 7: Clean data by replacing known bad values\n",
    "        exp_data_analysis.replace_values('Segment', '%', 'UNKNOWN')\n",
    "        exp_data_analysis.replace_values('Country', '56', 'UNKNOWN')\n",
    "        exp_data_analysis.replace_values('Quantity', 'Two', 2)\n",
    "        exp_data_analysis.replace_values('Quantity', 'Thirteen', 13)\n",
    "        exp_data_analysis.replace_values('Quantity', 'Seven', 7)\n",
    "        exp_data_analysis.replace_values('Quantity', 'ten', 10)\n",
    "        exp_data_analysis.replace_values('Quantity', '7?', 7)\n",
    "        \n",
    "        # Step 8: Re-check unique values after replacements\n",
    "        exp_data_analysis.print_uniques(categorical_columns)\n",
    "        \n",
    "        # Step 9: Convert categorical columns to 'category' type to optimize memory usage\n",
    "        exp_data_analysis.convert_columns_dtype(dict((x, 'category') for x in categorical_columns))\n",
    "        \n",
    "        # Step 10: Convert 'Profit' and 'Quantity' columns to float\n",
    "        exp_data_analysis.convert_columns_dtype({'Profit': 'float', 'Quantity': 'float'})\n",
    "        \n",
    "        # Step 11: Generate descriptive statistics\n",
    "        exp_data_analysis.descriptive_stats()\n",
    "        \n",
    "        # Step 12: Summarize sales and profit by category\n",
    "        print(\"\\nSales and Profit by Category:\")\n",
    "        print(exp_data_analysis.group_and_summarize(group_by_col=\"Category\", agg_cols=['Sales', 'Profit'], sort_by='Sales'))\n",
    "        \n",
    "        # Step 13: Summarize profit by region\n",
    "        print(\"\\nProfit by Region:\")\n",
    "        print(exp_data_analysis.group_and_summarize(group_by_col=\"Region\", agg_cols=\"Profit\", sort_by=\"Profit\"))\n",
    "\n",
    "    except ColumnNotFoundError as e:    \n",
    "        print(f\"\\nColumnNotFoundError: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c46ed-ba40-4f5b-87e9-5b4ce4f1e35b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
