{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdde4d21-760b-4cc1-93d5-766d7dceb73f",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1:\n",
    "A- Load the provided CSV dataset (sample-superstore.csv) into Python and print the first ten records with the associated column names.\n",
    "\n",
    "B- Provide a short paragraph to describe your understanding of the dataset. (around 100 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107961d-932f-47d6-af5f-6a657312f5f2",
   "metadata": {},
   "source": [
    "### Task 1 - Part A: Load and Explore the Dataset\n",
    "In this task, we’ll use the Pandas library to load the sample-superstore.csv dataset and define two reusable methods:\n",
    "- head(limit) – Returns the top N rows of the dataset.\n",
    "- tail(limit) – Returns the bottom N rows of the dataset.\n",
    "\n",
    "These methods take a parameter limit that specifies how many rows to return.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11902da1-3c41-4fce-91b2-5cc50f248b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   Row ID | Order ID       | Order Date   | Ship Date   | Ship Mode      | Customer ID   | Customer Name    | Segment     | Country       | City         | State          | Postal Code   | Region   | Product ID      | Category        | Sub-Category   | Product Name                                                                |    Sales | Quantity   |   Discount | Profit       |\n",
      "|---:|---------:|:---------------|:-------------|:------------|:---------------|:--------------|:-----------------|:------------|:--------------|:-------------|:---------------|:--------------|:---------|:----------------|:----------------|:---------------|:----------------------------------------------------------------------------|---------:|:-----------|-----------:|:-------------|\n",
      "|  0 |     7773 | CA-2016-108196 | 25/11/2016   | 12/02/2016  | Standard Class | CS-12505      | Cindy Stewart    | Consumer    | United States | Lancaster    | Ohio           | 43130         | Est      | TEC-MA-10000418 | Technology      | Machines       | Cubify CubeX 3D Printer Double Head Print                                   |  4499.98 | 5          |        0.7 | -6599.978    |\n",
      "|  1 |      684 | US-2017-168116 | 11/04/2017   | 11/04/2017  | Same Day       | GT-14635      | Grant Thornton   | Corporate   | United States | Burlington   | North Carolina | \"27217\"       | South    | TEC-MA-10004125 | Technology      | Machines       | Cubify CubeX 3D Printer Triple Head Print                                   |  7999.98 | 4          |        0.5 | -3839.9904   |\n",
      "|  2 |     9775 | CA-2014-169019 | 26/07/2014   | 30/07/2014  | Standard Class | LF-17185      | Luke Foster      | Consumer    | United States | San Antonio  | Texas          | 78207         | Central  | OFF-BI-10004995 | Office Supplies | Binders        | GBC DocuBind P400 Electric Binding System                                   |  2177.58 | 8          |        0.8 | -3701.8928   |\n",
      "|  3 |     3012 | CA-2017-134845 | 17/04/2017   | 24/04/2017  | Standard Class | SR-20425      | Sharelle Roach   | Home Office | United States | Louisville   | Colorado       | 80027         | West     | TEC-MA-10000822 | Technology      | Machines       | Lexmark MX611dhe Monochrome Laser Printer                                   |  2549.99 | 5          |        0.7 | -3399.98     |\n",
      "|  4 |     4992 | US-2017-122714 | 12/07/2017   | 13/12/2017  | Standard Class | HG-14965      | Henry Goldwyn    | Corporate   | United States | Chicago      | Illinois       | 60653         | Central  | OFF-BI-10001120 | Office Supplies | Binders        | Ibico EPK-21 Electric Binding System                                        |  1889.99 | 5          |        0.8 | -2929.4845   |\n",
      "|  5 |     3152 | CA-2015-147830 | 15/12/2015   | 18/12/2015  | First Class    | NF-18385      | Natalie Fritzler | Consumer    | United States | Newark       | Ohio           | 43055         | East     | TEC-MA-10000418 | Technology      | Machines       | Cubify CubeX 3D Printer Double Head Print                                   |  1799.99 | Two        |        0.7 | \"-2639.9912\" |\n",
      "|  6 |     5311 | CA-2017-131254 | 19/11/2017   | 21/11/2017  | First Class    | NC-18415      | Nathan Cano      | Consumer    | United States | Houston      | Texas          | 77095         | Central  | OFF-BI-10003527 | Office Supplies | Binders        | Fellowes PB500 Electric Punch Plastic Comb Binding Machine with Manual Bind |  1525.19 | 6          |        0.8 | -2287.782    |\n",
      "|  7 |     9640 | CA-2015-116638 | 28/01/2015   | nan         | Second Class   | JH-15985      | Joseph Holt      | Consumer    | United States | Concord      | North Carolina | 28027         | South    | FUR-TA-10000198 | Frnture         | Tables         | Chromcraft Bull-Nose Wood Oval Conference Tables & Bases                    |  4297.64 | Thirteen   |        0.4 | nan          |\n",
      "|  8 |     1200 | CA-2016-130946 | 04/08/2016   | 04/12/2016  | Standard Class | ZC-21910      | Zuschuss Carroll | Consumer    | United States | Houston      | Texas          | 77041         | Central  | OFF-BI-10004995 | Office Supplies | Binders        | GBC DocuBind P400 Electric Binding System                                   |  1088.79 | 4          |        0.8 | -1850.9464   |\n",
      "|  9 |     2698 | CA-2014-145317 | 18/03/2014   | 23/03/2014  | Standard Class | SM-20320      | Sean Miller      | Home Office | nan           | Jacksonville | Florida        | 32216         | Southh   | TEC-MA-10002412 | Technology      | Machines       | Cisco TelePresence System EX90 Videoconferencing Unit                       | 22638.5  | 6          |        0.5 | -1811.0784   |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "class ExploratoryDataAnalysis:\n",
    "    ## Class-level variables\n",
    "    store_data_frame: DataFrame = None\n",
    "    \n",
    "    def __init__(self, path: str):\n",
    "        self.store_data_frame = pd.read_csv(path, sep=',')\n",
    "\n",
    "    def head(self, limit: int):\n",
    "        return self.store_data_frame.head(limit)\n",
    "\n",
    "    def tail(self, limit: int):\n",
    "        return self.store_data_frame.tail(limit)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    exp_data_analysis = ExploratoryDataAnalysis('sample-superstore.csv')\n",
    "    ## printing first 10 records associated with column names\n",
    "    print(exp_data_analysis.head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75d86f-411b-404b-ac90-53a819fd8a68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### Task 1 Part B - Understanding the Sample Superstore Dataset\n",
    "\n",
    "The Sample Superstore dataset captures detailed retail sales data from a fictional store. It includes information about customer orders such as \n",
    "- `Order ID`\n",
    "- `Order Date`\n",
    "- `Ship Mode`\n",
    "- `Customer Name`\n",
    "- `Segment`\n",
    "- `City`\n",
    "- `State`\n",
    "- `Region`\n",
    "\n",
    "Each transaction is linked to a product with,\n",
    "- `Product ID`\n",
    "- `Category`\n",
    "- `Sub-Category`\n",
    "- `Product Name`\n",
    "\n",
    "Each transaction contains metrics such as \n",
    "- `sales`\n",
    "- `Quantity`\n",
    "- `Discount`\n",
    "- `Profit`\n",
    "\n",
    "This dataset is ideal for analysing customer purchasing behaviour, shipping performance, product profitability, and regional sales trends. It can be used in data science for performing exploratory data analysis (EDA), creating dashboards, and building predictive business models.\n",
    "\n",
    "### Loading Data\n",
    "To work with this data in Python, we use the Pandas library, which provides powerful tools for data manipulation and analysis. We load the dataset using `pd.read_csv()`, which reads the CSV file and returns a `DataFrame`. A DataFrame is a two-dimensional labelled data structure in Pandas, similar to a table in a database or an Excel spreadsheet. It allows us to easily inspect, filter, sort, and transform the data. Once loaded, the dataset becomes a DataFrame object where each row represents a single order and each column represents a different attribute related to the order.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cabb8e5-b3b5-40a9-9a37-18bddf565ac3",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "Process the dataset's variables and conduct exploratory data analysis. Explore the dataset thoroughly, and feel free to improvise as needed. However, you must use Python for at least four of the following techniques:\n",
    "\n",
    "- Descriptive statistics: Describe features of the data set by generating summaries about data samples.\n",
    "- Outlier treatment: Identify abnormal or problematic values and apply methods to treat them.\n",
    "- Normalising and scaling (numerical variables): Apply normalisation and scaling methods to transform data for further analysis.\n",
    "- Grouping of data: Demonstrate data aggregations or frequency distributions to summarise analysis.\n",
    "- Handling missing values in the dataset: Identify methods for cleaning the dataset.\n",
    "- Correlation: Describe features that are related and the nature of that relationship.\n",
    "- Univariate analysis and visualisation: Use different visualisation methods for demonstrating your analysis scenarios.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0b93733-067d-4210-8dd0-1c6734c18af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Dataset:  (9994, 21)\n",
      "\n",
      "Column Names:  ['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Quantity', 'Discount', 'Profit']\n",
      "\n",
      "Data types and non-null counts: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9994 entries, 0 to 9993\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Row ID         9994 non-null   int64  \n",
      " 1   Order ID       9993 non-null   object \n",
      " 2   Order Date     9992 non-null   object \n",
      " 3   Ship Date      9991 non-null   object \n",
      " 4   Ship Mode      9990 non-null   object \n",
      " 5   Customer ID    9994 non-null   object \n",
      " 6   Customer Name  9991 non-null   object \n",
      " 7   Segment        9991 non-null   object \n",
      " 8   Country        9990 non-null   object \n",
      " 9   City           9992 non-null   object \n",
      " 10  State          9990 non-null   object \n",
      " 11  Postal Code    9991 non-null   object \n",
      " 12  Region         9991 non-null   object \n",
      " 13  Product ID     9992 non-null   object \n",
      " 14  Category       9992 non-null   object \n",
      " 15  Sub-Category   9990 non-null   object \n",
      " 16  Product Name   9991 non-null   object \n",
      " 17  Sales          9993 non-null   float64\n",
      " 18  Quantity       9989 non-null   object \n",
      " 19  Discount       9991 non-null   float64\n",
      " 20  Profit         9983 non-null   object \n",
      "dtypes: float64(2), int64(1), object(18)\n",
      "memory usage: 1.6+ MB\n",
      "None\n",
      "\n",
      "Missing values in each column: \n",
      "Row ID            0\n",
      "Order ID          1\n",
      "Order Date        2\n",
      "Ship Date         3\n",
      "Ship Mode         4\n",
      "Customer ID       0\n",
      "Customer Name     3\n",
      "Segment           3\n",
      "Country           4\n",
      "City              2\n",
      "State             4\n",
      "Postal Code       3\n",
      "Region            3\n",
      "Product ID        2\n",
      "Category          2\n",
      "Sub-Category      4\n",
      "Product Name      3\n",
      "Sales             1\n",
      "Quantity          5\n",
      "Discount          3\n",
      "Profit           11\n",
      "dtype: int64\n",
      "\n",
      "Columns with ≤ 5 unique values (possible categorical features):\n",
      "\n",
      "['Ship Mode', 'Segment', 'Country', 'Category']\n",
      "\n",
      "Filling up missing values in categorical columns with value 'UNKNOWN'\n",
      "\n",
      "\n",
      "Column Ship Mode have values ['Standard Class' 'Same Day' 'First Class' 'Second Class' 'UNKNOWN'].\n",
      "Column Segment have values ['Consumer' 'Corporate' 'Home Office' '%' 'UNKNOWN'].\n",
      "Column Country have values ['United States' 'UNKNOWN' 'US' '56'].\n",
      "Column Category have values ['Technology' 'Office Supplies' 'Frnture' 'Furniture' 'UNKNOWN'].\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ExploratoryDataAnalysis.replace_values() missing 2 required positional arguments: 'currVal' and 'newVal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 286\u001b[0m\n\u001b[1;32m    283\u001b[0m exp_data_analysis\u001b[38;5;241m.\u001b[39mprint_uniques(categorical_columns)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# Step 7: Clean data by replacing known bad values\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m exp_data_analysis\u001b[38;5;241m.\u001b[39mreplace_values([\n\u001b[1;32m    287\u001b[0m     ColumnValueReplacer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSegment\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNKNOWN\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    288\u001b[0m     ColumnValueReplacer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m56\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNKNOWN\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    289\u001b[0m     ColumnValueReplacer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTwo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    290\u001b[0m     ColumnValueReplacer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThirteen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m13\u001b[39m),\n\u001b[1;32m    291\u001b[0m     ColumnValueReplacer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeven\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m7\u001b[39m),\n\u001b[1;32m    292\u001b[0m     ColumnValueReplacer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mten\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m    293\u001b[0m     ColumnValueReplacer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m7?\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m    294\u001b[0m ])\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Step 8: Re-check unique values after replacements\u001b[39;00m\n\u001b[1;32m    297\u001b[0m exp_data_analysis\u001b[38;5;241m.\u001b[39mprint_uniques(categorical_columns)\n",
      "\u001b[0;31mTypeError\u001b[0m: ExploratoryDataAnalysis.replace_values() missing 2 required positional arguments: 'currVal' and 'newVal'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from enum import Enum\n",
    "\n",
    "class ColumnValueReplacer:\n",
    "    def __init__(self, column, curr_value, new_value):\n",
    "        self.column = column\n",
    "        self.curr_value = curr_value\n",
    "        self.new_value = new_value\n",
    "\n",
    "class OutlierRemovalMethods(Enum):\n",
    "    REMOVE = 1\n",
    "    CAP = 2\n",
    "\n",
    "class ColumnTypeConversionFailureException(Exception):\n",
    "    def __init__(self, message):\n",
    "        super().__init__(message)\n",
    "    \n",
    "class ColumnNotFoundError(Exception):\n",
    "    def __init__(self, missing_columns):\n",
    "        message = f\"The following column(s) are missing: {missing_columns}\"\n",
    "        super().__init__(message)\n",
    "        self.missing_columns = missing_columns\n",
    "\n",
    "class ExploratoryDataAnalysis:\n",
    "    ## Class-level variables\n",
    "    store_data_frame: DataFrame = None\n",
    "    \n",
    "    def __init__(self, path: str):\n",
    "        self.store_data_frame = pd.read_csv(path, sep=',')\n",
    "\n",
    "    def inspect(self):\n",
    "        \"\"\"\n",
    "        This method will provide inspection for the store data frame.\n",
    "        \"\"\"\n",
    "        print(\"Shape of Dataset: \", self.store_data_frame.shape)\n",
    "        print(\"\\nColumn Names: \", self.store_data_frame.columns.tolist())\n",
    "\n",
    "    def descriptive_stats(self):\n",
    "        \"\"\"\n",
    "        This method will provide descriptive statistics for the store data frame.\n",
    "        \"\"\"\n",
    "        print(\"\\nDescriptive Statistics: \", self.store_data_frame.describe())\n",
    "\n",
    "    def basic_info(self):\n",
    "        \"\"\"\n",
    "        This method will provide basic information for the store data frame.\n",
    "        \"\"\"\n",
    "        print(\"\\nData types and non-null counts: \")\n",
    "        print(self.store_data_frame.info())\n",
    "\n",
    "    def missing_value_info(self):\n",
    "        \"\"\"\n",
    "        This method will provide missing value information for the store data frame.\n",
    "        \"\"\"\n",
    "        print(\"\\nMissing values in each column: \")\n",
    "        print(self.store_data_frame.isnull().sum())\n",
    "\n",
    "    def get_categorical_candidates(self, threshold=10):\n",
    "        \"\"\"\n",
    "        Return columns that have unique values less than or equal to `threshold`.\n",
    "        These are good candidates for categorical analysis.\n",
    "        \"\"\"\n",
    "        candidate_columns = []\n",
    "        \n",
    "        print(f\"\\nColumns with ≤ {threshold} unique values (possible categorical features):\\n\")\n",
    "        for col in self.store_data_frame.columns:\n",
    "            unique_values = self.store_data_frame[col].nunique()\n",
    "            if unique_values <= threshold:\n",
    "                candidate_columns.append((col, unique_values))\n",
    "\n",
    "        return [col for col, _ in candidate_columns]\n",
    "\n",
    "    def fill_missing_values(self, categorical_columns, fill_value):\n",
    "        \"\"\"\n",
    "        This method will fill all missing values for categorical columns with the provided fill_value.\n",
    "        \"\"\"\n",
    "        missing = [col for col in categorical_columns if col not in self.store_data_frame.columns]\n",
    "        if missing:\n",
    "            raise ColumnNotFoundError(missing)\n",
    "            \n",
    "        print(f\"\\nFilling up missing values in categorical columns with value '{fill_value}'\")\n",
    "        for col in categorical_columns:\n",
    "            self.store_data_frame[col] = self.store_data_frame[col].fillna(fill_value)\n",
    "\n",
    "    def print_uniques(self, columns = None):\n",
    "        \"\"\"\n",
    "        This method is used to print unique values of the provided columns.\n",
    "        \"\"\"\n",
    "        missing = [col for col in categorical_columns if col not in self.store_data_frame.columns]\n",
    "        if missing:\n",
    "            raise ColumnNotFoundError(missing)\n",
    "         \n",
    "        print(\"\\n\")   \n",
    "        if columns:\n",
    "            for col in columns:\n",
    "                print(f\"Column {col} have values {self.store_data_frame[col].unique()}.\")\n",
    "        else:\n",
    "            print(self.store_data_frame.head(5))\n",
    "\n",
    "    def replace_values(self, column_value_replacers: list[ColumnValueReplacer]):\n",
    "        \"\"\"\n",
    "        Replaces specific values in one or more DataFrame columns.\n",
    "\n",
    "        Parameters:\n",
    "            column_value_replacers (list[ColumnValueReplacer]):\n",
    "                A list of ColumnValueReplacer instances containing:\n",
    "                - column: Name of the column to update.\n",
    "                - curr_value: Current (old) value to be replaced.\n",
    "                - new_value: New value to replace with.\n",
    "\n",
    "        Raises:\n",
    "            ColumnNotFoundError: If any specified column is not found in the DataFrame.\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting value replacements...\\n\")\n",
    "\n",
    "        for replacer in column_value_replacers:\n",
    "            if replacer.column not in self.store_data_frame.columns:\n",
    "                raise ColumnNotFoundError(f\"Column '{replacer.column}' does not exist in the DataFrame.\")\n",
    "\n",
    "            # Count before replacement\n",
    "            count_before = (self.store_data_frame[replacer.column] == replacer.curr_value).sum()\n",
    "\n",
    "            # Perform replacement\n",
    "            self.store_data_frame[replacer.column] = self.store_data_frame[replacer.column].replace(\n",
    "                replacer.curr_value, replacer.new_value\n",
    "            )\n",
    "\n",
    "            # Count after replacement\n",
    "            count_after = (self.store_data_frame[replacer.column] == replacer.new_value).sum()\n",
    "\n",
    "            print(\n",
    "                f\"Replaced {count_before} occurrence(s) of '{replacer.curr_value}' \"\n",
    "                f\"with '{replacer.new_value}' in column '{replacer.column}'.\"\n",
    "            )\n",
    "            print(f\"Total now: {count_after} instance(s) of '{replacer.new_value}' in '{replacer.column}'.\\n\")\n",
    "\n",
    "    def convert_columns_dtype(self, dtype_map):\n",
    "        \"\"\"\n",
    "        Convert specified columns to desired data types.\n",
    "\n",
    "        Parameters:\n",
    "        - dtype_map (dict): A dictionary where keys are column names and values are target data types.\n",
    "                        Example: {'Segment': 'category', 'Sales': 'float'}\n",
    "        \"\"\"\n",
    "        print(\"\\n\")\n",
    "        for col, dtype in dtype_map.items():\n",
    "            if col not in self.store_data_frame.columns:\n",
    "                raise ColumnNotFoundError(col)\n",
    "            try:\n",
    "                self.store_data_frame[col] = (self.store_data_frame[col].astype(str).str.replace('\"', '', regex=False).str.strip())\n",
    "                self.store_data_frame[col] = self.store_data_frame[col].astype(dtype)\n",
    "                print(f\"Column {col} type is converted to {dtype}\")\n",
    "            except Exception as ex:\n",
    "                error_message = f\"Failed to convert column '{col}' to '{dtype}': {str(ex)}\"\n",
    "                raise ColumnTypeConversionFailureException(error_message)\n",
    "\n",
    "    def group_and_summarize(self, group_by_col, agg_cols, sort_by=None, ascending=False):\n",
    "        \"\"\"\n",
    "        Groups a DataFrame by one or more columns and summarises given numeric columns.\n",
    "\n",
    "        Parameters:\n",
    "            df (DataFrame): The input DataFrame.\n",
    "            group_by_col (str or list): Column(s) to group by.\n",
    "            agg_cols (str or list): Column(s) to aggregate (e.g., 'Sales', 'Profit').\n",
    "            sort_by (str): Column to sort the result by (must be in agg_cols).\n",
    "            ascending (bool): Sort order. The default is descending.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: Grouped and aggregated DataFrame.\n",
    "        \"\"\"\n",
    "        if isinstance(agg_cols, str):\n",
    "            agg_cols = [agg_cols]\n",
    "\n",
    "        result = self.store_data_frame.groupby(group_by_col, observed=False)[agg_cols].sum()\n",
    "        if sort_by:\n",
    "            result = result.sort_values(by=sort_by, ascending=ascending)\n",
    "        return result\n",
    "\n",
    "    def plot_boxplot(self, column):\n",
    "        \"\"\"\n",
    "        Plot a styled boxplot for the specified column.\n",
    "        \"\"\"\n",
    "        if column not in self.store_data_frame.columns:\n",
    "            raise ColumnNotFoundError(f\"Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(\n",
    "            x=self.store_data_frame[column],\n",
    "            color=\"skyblue\",\n",
    "            linewidth=2,\n",
    "            fliersize=5  # size of outlier points\n",
    "        )\n",
    "        plt.title(f\"Boxplot of '{column}'\", fontsize=16, fontweight='bold')\n",
    "        plt.xlabel(column, fontsize=14)\n",
    "        plt.ylabel(\"Frequency\", fontsize=14)\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def treat_outliers(self, columns, method=OutlierRemovalMethods.REMOVE):\n",
    "        \"\"\"\n",
    "        Detect and treat outliers using IQR method.\n",
    "    \n",
    "        Parameters:\n",
    "        columns (str): Columns to process\n",
    "        method (OutlierRemovalMethods): REMOVE to drop outliers, CAP to clip to IQR bounds\n",
    "        \"\"\"\n",
    "        for col in columns:\n",
    "            if col not in self.store_data_frame.columns:\n",
    "                raise ColumnNotFoundError(f\"Column '{col}' not found in DataFrame.\")\n",
    "            \n",
    "            bounds = self.get_bounds(col)   \n",
    "            print(f\"\\n{col}: Detected {bounds} bounds using IQR.\")\n",
    "            \n",
    "            if method == OutlierRemovalMethods.REMOVE:\n",
    "                self.store_data_frame = self.store_data_frame[\n",
    "                    (self.store_data_frame[col] >= bounds[0]) &\n",
    "                    (self.store_data_frame[col] <= bounds[1])\n",
    "                ]\n",
    "            elif method == OutlierRemovalMethods.CAP:\n",
    "                self.store_data_frame = np.where(\n",
    "                    self.store_data_frame[col] < bounds[0], bounds[0],\n",
    "                    np.where(self.store_data_frame[col] > bounds[1], bounds[1], self.store_data_frame[col])\n",
    "                )\n",
    "                \n",
    "    \n",
    "    def get_bounds(self, column):\n",
    "        \"\"\"\n",
    "        Detect and optionally treat outliers in a column using IQR(Interquartile range) method.\n",
    "        \"\"\"\n",
    "        q1 = self.store_data_frame[column].quantile(0.25)\n",
    "        q3 = self.store_data_frame[column].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        return [lower_bound, upper_bound]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        exp_data_analysis = ExploratoryDataAnalysis('sample-superstore.csv')\n",
    "        \n",
    "        # Step 1: Load and inspect the dataset\n",
    "        exp_data_analysis.inspect()\n",
    "        \n",
    "        # Step 2: Display data types and basic structure\n",
    "        exp_data_analysis.basic_info()\n",
    "        \n",
    "        # Step 3: Analyse missing values\n",
    "        exp_data_analysis.missing_value_info()\n",
    "        \n",
    "        # Step 4: Identify candidate categorical columns (based on unique value count)\n",
    "        categorical_columns = exp_data_analysis.get_categorical_candidates(5)\n",
    "        print(categorical_columns)\n",
    "        \n",
    "        # Step 5: Fill missing values in categorical columns with \"UNKNOWN\"\n",
    "        exp_data_analysis.fill_missing_values(categorical_columns, \"UNKNOWN\")\n",
    "        \n",
    "        # Step 6: Show unique values in categorical columns after filling\n",
    "        exp_data_analysis.print_uniques(categorical_columns)\n",
    "        \n",
    "        # Step 7: Clean data by replacing known bad values\n",
    "        exp_data_analysis.replace_values([\n",
    "            ColumnValueReplacer('Segment', '%', 'UNKNOWN'),\n",
    "            ColumnValueReplacer('Country', '56', 'UNKNOWN'),\n",
    "            ColumnValueReplacer('Quantity', 'Two', 2),\n",
    "            ColumnValueReplacer('Quantity', 'Thirteen', 13),\n",
    "            ColumnValueReplacer('Quantity', 'Seven', 7),\n",
    "            ColumnValueReplacer('Quantity', 'ten', 10),\n",
    "            ColumnValueReplacer('Quantity', '7?', 7)\n",
    "        ])\n",
    "        \n",
    "        # Step 8: Re-check unique values after replacements\n",
    "        exp_data_analysis.print_uniques(categorical_columns)\n",
    "        \n",
    "        # Step 9: Convert categorical columns to 'category' type to optimise memory usage\n",
    "        exp_data_analysis.convert_columns_dtype(dict((x, 'category') for x in categorical_columns))\n",
    "        \n",
    "        # Step 10: Convert 'Profit' and 'Quantity' columns to float\n",
    "        exp_data_analysis.convert_columns_dtype({'Profit': 'float', 'Quantity': 'float'})\n",
    "        \n",
    "        # Step 11: Generate descriptive statistics\n",
    "        exp_data_analysis.descriptive_stats()\n",
    "        \n",
    "        # Step 12: Summarise sales and profit by category\n",
    "        print(\"\\nSales and Profit by Category:\")\n",
    "        print(exp_data_analysis.group_and_summarize(group_by_col=\"Category\", agg_cols=['Sales', 'Profit'], sort_by='Sales'))\n",
    "        \n",
    "        # Step 13: Summarise profit by region\n",
    "        print(\"\\nProfit by Region:\")\n",
    "        print(exp_data_analysis.group_and_summarize(group_by_col=\"Region\", agg_cols=\"Profit\", sort_by=\"Profit\"))\n",
    "\n",
    "        # Step 14: BoxPlot of Quantity to visualise outliers\n",
    "        exp_data_analysis.plot_boxplot('Quantity')\n",
    "\n",
    "        # Step 15: BoxPlot of Sales to visualise outliers\n",
    "        exp_data_analysis.plot_boxplot('Sales')  \n",
    "        \n",
    "        # Step 16: Treat Outliers of Quantity and Sales\n",
    "        exp_data_analysis.treat_outliers(['Quantity', 'Sales'])\n",
    "\n",
    "        print(\"\\nPlotting quantity and sales after treating with outliers\")\n",
    "        \n",
    "        # Step 17: BoxPlot of Quantity to visualise outliers\n",
    "        exp_data_analysis.plot_boxplot('Quantity')\n",
    "\n",
    "        # Step 18: BoxPlot of Sales to visualise outliers\n",
    "        exp_data_analysis.plot_boxplot('Sales') \n",
    "\n",
    "    except ColumnNotFoundError as e:    \n",
    "        print(f\"\\nColumnNotFoundError: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c6e77-c831-479a-ae53-f343f98ad1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
